<!--
 * @LastEditors: jingweizhu
-->
# Softmax函数
- softmax函数，又称归一化指数函数
- 是二分类函数 sigmoid 在多分类上的推广
- 目的是，将多分类的结果以概率的形式展现出来
- 举个例子，假如模型对一个三分类的预测结果为 -3，1.5，2.7，我们要用softmax将模型结果转为概率。步骤如下：
    * 将预测结果转化为非负数，y1 = exp(x1) = exp(-3) = 0.05; y2 = exp(x2) = exp(1.5) = 4.48; y3 = exp(x3) = exp(2.7) = 14.88;
    * 求和 sum = 0.05 + 4.48 + 14.88 = 19.41
    * z1 = 0.05/19.41 = 0.0026; z2 = 4.48/19.41 = 0.2308; z3 = 14.88/19.41 = 0.7666;
    * 验证 0.0026 + 0.2308 + 0.7666 = 1

# 矩阵乘法
## 矩阵叉乘，也叫外积   
- AB相乘时要求A的列数等于B的行数，
- 结果仍然为一个矩阵
- shape会变，m*n的矩阵 乘以 n*x 的矩阵，最后变成 m*x 的矩阵
- 方法：numpy里面对应 dot 或者 matmul 函数
## 矩阵点乘，也叫内积
- AB相乘时要求A的行数等于B的行数，A的列数等于B的列数，AB属于同型矩阵
- 结果是一个值，或者叫标量，是矩阵对应元素乘积之和 
## element-wise乘法
- AB相乘时要求A的行数等于B的行数，A的列数等于B的列数，AB属于同型矩阵
- 对应元素分别相乘，前后shape不变，两个 m*n 的 矩阵相乘仍然是一个 m*n 的矩阵
- 方法：a * b 或者 a.multiply(b) 或者 np.multiply(a, b)

# 卷积计算 Convolution
- 卷积核kernel，也叫滤波器filter
- 关于卷积的计算，这篇文档可以说是非常形象了： https://www.paddlepaddle.org.cn/tutorials/projectdetail/1332782
- 

# 池化 pooling

# 常见的激活函数
- Sigmoid
- ReLU

# Dropout
- 丢弃法 dropout 是深度学习中一种常见的抑制过拟合的方法
- 其做法是随机删除一部分神经元。随机选出一部分神经元，将其输出设置为0，这些神经元不对外传递信号

